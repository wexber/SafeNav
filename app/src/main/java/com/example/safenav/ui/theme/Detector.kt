package com.example.safenav.ui.theme

import android.content.ContentValues.TAG
import android.content.Context
import android.graphics.Bitmap
import android.os.SystemClock
import android.os.Handler
import android.os.Looper
import android.speech.tts.TextToSpeech
import android.util.Log
import org.tensorflow.lite.DataType
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.support.common.FileUtil
import org.tensorflow.lite.support.common.ops.CastOp
import org.tensorflow.lite.support.common.ops.NormalizeOp
import org.tensorflow.lite.support.image.ImageProcessor
import org.tensorflow.lite.support.image.TensorImage
import org.tensorflow.lite.support.tensorbuffer.TensorBuffer
import java.io.BufferedReader
import java.io.IOException
import java.io.InputStream
import java.io.InputStreamReader
import java.util.Locale

class Detector(
    private val context: Context,
    private val modelPath: String,
    private val labelPath: String,
    private val detectorListener: DetectorListener
) {

    // Agrega una lista de oyentes (listeners) para notificar cuando el intérprete está en ejecución
    private val interpreterListeners = mutableListOf<InterpreterListener>()



    interface InterpreterListener {
        fun onInterpreterCompleted()
        fun onInterpreterRunning()
        fun onInterpreterStart()
        fun onInterpreterError(error: String)
    }

    private var interpreter: Interpreter? = null
    private var labels = mutableListOf<String>()
    private var detectionInProgress = false
    private var stopRequested = false
    private var tensorWidth = 0
    private var tensorHeight = 0
    private var numChannel = 0
    private var numElements = 0

    private val imageProcessor = ImageProcessor.Builder()
        .add(NormalizeOp(INPUT_MEAN, INPUT_STANDARD_DEVIATION))
        .add(CastOp(INPUT_IMAGE_TYPE))
        .build()
    private val uiHandler = Handler(Looper.getMainLooper())
    // Agrega una instancia de TextToSpeech
    private var textToSpeech: TextToSpeech? = null
    fun setup() {
        val model = FileUtil.loadMappedFile(context, modelPath)
        val options = Interpreter.Options()
        options.numThreads = 6
        interpreter = Interpreter(model, options)

        val inputShape = interpreter?.getInputTensor(0)?.shape() ?: return
        val outputShape = interpreter?.getOutputTensor(0)?.shape() ?: return

        tensorWidth = inputShape[1]
        tensorHeight = inputShape[2]
        numChannel = outputShape[1]
        numElements = outputShape[2]

        try {
            val inputStream: InputStream = context.assets.open(labelPath)
            val reader = BufferedReader(InputStreamReader(inputStream))

            var line: String? = reader.readLine()
            while (line != null && line != "") {
                labels.add(line)
                line = reader.readLine()
            }

            reader.close()
            inputStream.close()
        } catch (e: IOException) {
            e.printStackTrace()
        }

        // Inicializar TextToSpeech
        textToSpeech = TextToSpeech(context) { status ->
            if (status == TextToSpeech.SUCCESS) {
                val result = textToSpeech?.setLanguage(Locale.getDefault())
                if (result == TextToSpeech.LANG_MISSING_DATA || result == TextToSpeech.LANG_NOT_SUPPORTED) {
                    Log.e("TTS", "El idioma no es compatible o falta datos de voz")
                }
            } else {
                Log.e("TTS", "Error al inicializar TextToSpeech")
            }
        }
    }

    fun clear() {

        // Limpiar el TextToSpeech
        textToSpeech?.stop()
        textToSpeech?.shutdown()
        textToSpeech = null

        // Limpiar el Handler
        uiHandler.removeCallbacksAndMessages(null)

        // Cerrar el intérprete de TensorFlow Lite de forma segura
        interpreter?.close()
        interpreter = null
        // Detener la cámara si se está utilizando para la detección de objetos

    }
/*
    // Dentro de la función detect()
    fun detect(frame: Bitmap) {
        interpreter ?: return
        if (tensorWidth == 0) return
        if (tensorHeight == 0) return
        if (numChannel == 0) return
        if (numElements == 0) return

        val inferenceTime = SystemClock.uptimeMillis()

        val resizedBitmap = Bitmap.createScaledBitmap(frame, tensorWidth, tensorHeight, false)

        val tensorImage = TensorImage(DataType.FLOAT32)
        tensorImage.load(resizedBitmap)
        val processedImage = imageProcessor.process(tensorImage)
        val imageBuffer = processedImage.buffer

        val output = TensorBuffer.createFixedSize(intArrayOf(1, numChannel, numElements), OUTPUT_IMAGE_TYPE)
        interpreter?.run(imageBuffer, output.buffer)

        val bestBoxes = bestBox(output.floatArray)
        val elapsedTime = SystemClock.uptimeMillis() - inferenceTime


        uiHandler.post {
            if (bestBoxes.isNullOrEmpty()) {
                detectorListener.onEmptyDetect()
            } else {
                detectorListener.onDetect(bestBoxes, elapsedTime)
            }
        }
    }



 */

/*
    fun detect(frame: Bitmap) {
        try {
            Log.d(TAG, "Iniciando detección...")
            interpreter ?: return
            if (tensorWidth == 0 || tensorHeight == 0 || numChannel == 0 || numElements == 0) return
            Log.e(TAG, "Dimensiones de tensor inválidas")
            val inferenceTime = SystemClock.uptimeMillis()

            val resizedBitmap = Bitmap.createScaledBitmap(frame, tensorWidth, tensorHeight, false)

            val tensorImage = TensorImage(DataType.FLOAT32)
            tensorImage.load(resizedBitmap)
            val processedImage = imageProcessor.process(tensorImage)
            val imageBuffer = processedImage.buffer

            val output = TensorBuffer.createFixedSize(intArrayOf(1, numChannel, numElements), OUTPUT_IMAGE_TYPE)
            synchronized(interpreterLock) {
                // Ejecutar el intérprete de TensorFlow Lite
                Log.d(TAG, "Ejecutando el intérprete...")
                interpreter?.run(imageBuffer, output.buffer)
                Log.d(TAG, "Intérprete completado")
            }
            val bestBoxes = bestBox(output.floatArray)
            val elapsedTime = SystemClock.uptimeMillis() - inferenceTime

            uiHandler.post {
                if (bestBoxes.isNullOrEmpty()) {
                    detectorListener.onEmptyDetect()
                } else {
                    detectorListener.onDetect(bestBoxes, elapsedTime)
                }
            }
            Log.d(TAG, "Detección completada")
        } catch (e: Exception) {
            Log.e(TAG, "Error durante la inferencia: ${e.message}", e)
        }
    }

 */

    fun detect(frame: Bitmap) {
        if (stopRequested) {
            Log.d(TAG, "Detección detenida")
            return
        }

        synchronized(interpreterLock) {
            if (detectionInProgress) {
                Log.d(TAG, "Detección en curso, ignora la nueva solicitud")
                return
            }

            interpreter ?: return
            if (tensorWidth == 0 || tensorHeight == 0 || numChannel == 0 || numElements == 0) return
            val inferenceTime = SystemClock.uptimeMillis()

            val resizedBitmap = Bitmap.createScaledBitmap(frame, tensorWidth, tensorHeight, false)
            val tensorImage = TensorImage(DataType.FLOAT32)
            tensorImage.load(resizedBitmap)
            val processedImage = imageProcessor.process(tensorImage)
            val imageBuffer = processedImage.buffer

            // Initialize output tensor
            val output = TensorBuffer.createFixedSize(intArrayOf(1, numChannel, numElements), OUTPUT_IMAGE_TYPE)

            detectionInProgress = true

            try {
                Log.d(TAG, "Ejecutando el intérprete...")
                interpreter?.run(imageBuffer, output.buffer)
                Log.d(TAG, "Intérprete completado")

                val bestBoxes = bestBox(output.floatArray)
                val elapsedTime = SystemClock.uptimeMillis() - inferenceTime

                uiHandler.post {
                    if (bestBoxes.isNullOrEmpty()) {
                        detectorListener.onEmptyDetect()
                    } else {
                        detectorListener.onDetect(bestBoxes, elapsedTime)
                    }
                }
                Log.d(TAG, "Detección completada")
            } catch (e: IllegalStateException) {
                Log.e(TAG, "Error durante la inferencia: ${e.message}", e)
                notifyInterpreterError(e.message ?: "Error desconocido")
            } finally {
                detectionInProgress = false
                if (stopRequested) {
                    notifyInterpreterCompleted()
                }
            }
        }
    }

    fun isDetectionInProgress(): Boolean {
        return detectionInProgress
    }
    private fun bestBox(array: FloatArray) : List<BoundingBox>? {

        val boundingBoxes = mutableListOf<BoundingBox>()

        for (c in 0 until numElements) {
            var maxConf = -1.0f
            var maxIdx = -1
            var j = 4
            var arrayIdx = c + numElements * j
            while (j < numChannel){
                if (array[arrayIdx] > maxConf) {
                    maxConf = array[arrayIdx]
                    maxIdx = j - 4
                }
                j++
                arrayIdx += numElements
            }

            if (maxConf > CONFIDENCE_THRESHOLD) {
                val clsName = labels[maxIdx]
                val cx = array[c] // 0
                val cy = array[c + numElements] // 1
                val w = array[c + numElements * 2]
                val h = array[c + numElements * 3]
                val x1 = cx - (w/2F)
                val y1 = cy - (h/2F)
                val x2 = cx + (w/2F)
                val y2 = cy + (h/2F)
                if (x1 < 0F || x1 > 1F) continue
                if (y1 < 0F || y1 > 1F) continue
                if (x2 < 0F || x2 > 1F) continue
                if (y2 < 0F || y2 > 1F) continue

                boundingBoxes.add(
                    BoundingBox(
                        x1 = x1, y1 = y1, x2 = x2, y2 = y2,
                        cx = cx, cy = cy, w = w, h = h,
                        cnf = maxConf, cls = maxIdx, clsName = clsName
                    )
                )
            }
        }

        if (boundingBoxes.isEmpty()) return null

        return applyNMS(boundingBoxes)
    }

    private fun applyNMS(boxes: List<BoundingBox>) : MutableList<BoundingBox> {
        val sortedBoxes = boxes.sortedByDescending { it.cnf }.toMutableList()
        val selectedBoxes = mutableListOf<BoundingBox>()

        while(sortedBoxes.isNotEmpty()) {
            val first = sortedBoxes.first()
            selectedBoxes.add(first)
            sortedBoxes.remove(first)

            val iterator = sortedBoxes.iterator()
            while (iterator.hasNext()) {
                val nextBox = iterator.next()
                val iou = calculateIoU(first, nextBox)
                if (iou >= IOU_THRESHOLD) {
                    iterator.remove()
                }
            }
        }

        return selectedBoxes
    }

    private fun calculateIoU(box1: BoundingBox, box2: BoundingBox): Float {
        val x1 = maxOf(box1.x1, box2.x1)
        val y1 = maxOf(box1.y1, box2.y1)
        val x2 = minOf(box1.x2, box2.x2)
        val y2 = minOf(box1.y2, box2.y2)
        val intersectionArea = maxOf(0F, x2 - x1) * maxOf(0F, y2 - y1)
        val box1Area = box1.w * box1.h
        val box2Area = box2.w * box2.h
        return intersectionArea / (box1Area + box2Area - intersectionArea)
    }

    interface DetectorListener {
        fun onEmptyDetect()
        fun onDetect(boundingBoxes: List<BoundingBox>, inferenceTime: Long)
    }


    // Método para notificar a todos los oyentes cuando el intérprete está en ejecución
    private fun notifyInterpreterRunning() {
        for (listener in interpreterListeners) {
            listener.onInterpreterRunning()
        }
    }

    // Método para notificar a todos los oyentes cuando el intérprete ha completado su ejecución
    private fun notifyInterpreterCompleted() {
        for (listener in interpreterListeners) {
            listener.onInterpreterCompleted()
        }
    }
    private fun notifyInterpreterError(error: String) {
        for (listener in interpreterListeners) {
            listener.onInterpreterError(error)
        }
    }
    fun registerInterpreterListener(listener: InterpreterListener) {
        interpreterListeners.add(listener)
    }

    fun unregisterInterpreterListener(listener: InterpreterListener) {
        interpreterListeners.remove(listener)
    }

    // Método para iniciar el intérprete
    fun startInterpreter() {
        // Lógica para iniciar el intérprete...

        // Notificar a los oyentes que el intérprete está en ejecución
        notifyInterpreterRunning()
    }

    // Método para detener el intérprete
    fun stopInterpreter() {
        stopRequested = true
        if (!detectionInProgress) {
            notifyInterpreterCompleted()
        }
    }



    companion object {
        private const val INPUT_MEAN = 0f
        private const val INPUT_STANDARD_DEVIATION = 255f
        private val INPUT_IMAGE_TYPE = DataType.FLOAT32
        private val OUTPUT_IMAGE_TYPE = DataType.FLOAT32
        private const val CONFIDENCE_THRESHOLD = 0.5F
        private const val IOU_THRESHOLD = 0.5F
        private val interpreterLock = Any()
    }

}
